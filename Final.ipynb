{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0581f836-28dc-4ebe-bda2-974a0c180870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Loading BLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:37<00:00, 18.60s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading CLAP model...\n",
      "[INFO] Loading ResNet18 model...\n",
      "[INFO] Detected 4 emotion classes from checkpoint\n",
      "[INFO] Loaded pre-trained emotion model\n",
      "[INFO] Loading LLM model...\n",
      "WARNING:tensorflow:From C:\\Users\\DKpro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting video analysis for: dog.mp4\n",
      "[INFO] Extracting components from dog.mp4\n",
      "[INFO] Extracted 14 frames\n",
      "[WARNING] Could not extract audio: \n",
      "[INFO] Analyzing scenes with BLIP...\n",
      "[INFO] Frame 1: a person walking with a dog in a park\n",
      "\n",
      "[INFO] Frame 2: a woman is petting a golden retriever in a park\n",
      "\n",
      "[INFO] Frame 3: a person standing next to a dog in a park\n",
      "\n",
      "[INFO] Frame 4: a person walking with a dog in a park\n",
      "\n",
      "[INFO] Frame 5: a woman is playing with a dog in a park\n",
      "\n",
      "[INFO] Frame 6: a golden retriever is playing with its owner in a park\n",
      "\n",
      "[INFO] Frame 7: a person walking with a golden retriever in a park\n",
      "\n",
      "[INFO] Frame 8: a woman is playing with a dog in a park\n",
      "\n",
      "[INFO] Frame 9: a golden retriever is playing with a person in a park\n",
      "\n",
      "[INFO] Frame 10: golden retriever playing with his owner in the park\n",
      "\n",
      "[INFO] Frame 11: a woman is playing with a dog in a park\n",
      "\n",
      "[INFO] Frame 12: a person walking with a dog in a park\n",
      "\n",
      "[INFO] Frame 13: a golden retriever playing with a frisbee in a park\n",
      "\n",
      "[INFO] Frame 14: a person petting a dog in a park\n",
      "\n",
      "[INFO] Analyzing emotions with ResNet18...\n",
      "[INFO] Frame 1: angry (0.725)\n",
      "[INFO] Frame 2: angry (0.880)\n",
      "[INFO] Frame 3: angry (0.516)\n",
      "[INFO] Frame 4: angry (0.983)\n",
      "[INFO] Frame 5: sad (0.659)\n",
      "[INFO] Frame 6: angry (0.846)\n",
      "[INFO] Frame 7: angry (0.958)\n",
      "[INFO] Frame 8: sad (0.597)\n",
      "[INFO] Frame 9: angry (0.767)\n",
      "[INFO] Frame 10: angry (0.947)\n",
      "[INFO] Frame 11: sad (0.640)\n",
      "[INFO] Frame 12: angry (0.790)\n",
      "[INFO] Frame 13: sad (0.900)\n",
      "[INFO] Frame 14: sad (0.603)\n",
      "[INFO] Analyzing audio with CLAP...\n",
      "[INFO] Synthesizing analysis...\n",
      "[INFO] Video analysis completed!\n",
      "\n",
      "==================================================\n",
      "🐕 DOG ANALYSIS SUMMARY\n",
      "==================================================\n",
      "Mood: defensive\n",
      "Dominant Emotion: angry\n",
      "Audio Behavior: Silent\n",
      "Needs Attention: True\n",
      "Behavioral Interpretation: The dog is displaying defensive or aggressive behavior.\n",
      "\n",
      "Likely Thoughts:\n",
      "  - I feel threatened\n",
      "  - Stay away from me\n",
      "  - I need to defend myself\n",
      "  - My human is here!\n",
      "  - Time for adventure!\n",
      "[INFO] Results saved to dog_analysis_results.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "视频分析统合系统\n",
    "将视频分解为图片和音频，使用BLIP、CLAP和ResNet18进行分析，\n",
    "然后统合结果推测狗的想法和心情\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig, \n",
    "    Blip2Processor, \n",
    "    Blip2ForConditionalGeneration,\n",
    "    ClapModel, \n",
    "    ClapProcessor,\n",
    "    pipeline\n",
    ")\n",
    "from torchvision import transforms, models\n",
    "from torch import nn\n",
    "import tempfile\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class VideoAnalysisSystem:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"[INFO] Using device: {self.device}\")\n",
    "        \n",
    "        # 初始化所有模型\n",
    "        self._init_blip_model()\n",
    "        self._init_clap_model()\n",
    "        self._init_resnet_model()\n",
    "        self._init_llm_model()\n",
    "        \n",
    "    def _init_blip_model(self):\n",
    "        \"\"\"初始化BLIP模型用于场景描述\"\"\"\n",
    "        print(\"[INFO] Loading BLIP model...\")\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True,\n",
    "        )\n",
    "        \n",
    "        self.blip_processor = Blip2Processor.from_pretrained(\n",
    "            \"Salesforce/blip2-opt-2.7b\", use_fast=True\n",
    "        )\n",
    "        self.blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/blip2-opt-2.7b\",\n",
    "            quantization_config=quant_config,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self.blip_model.eval()\n",
    "        \n",
    "    def _init_clap_model(self):\n",
    "        \"\"\"初始化CLAP模型用于音频分析\"\"\"\n",
    "        print(\"[INFO] Loading CLAP model...\")\n",
    "        self.clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-fused\").to(self.device)\n",
    "        self.clap_processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n",
    "        \n",
    "        # 定义音频分析的提示词\n",
    "        self.audio_prompts = {\n",
    "            \"Alert\": [\n",
    "                \"sharp mid-pitch alert bark\",\n",
    "                \"brief clear alarm bark\",\n",
    "                \"short crisp warning bark\",\n",
    "                \"fast staccato alert bark\"\n",
    "            ],\n",
    "            \"Territorial\": [\n",
    "                \"deep sustained territorial bark\",\n",
    "                \"low throaty guard bark\",\n",
    "                \"prolonged defensive bark\",\n",
    "                \"slow booming territorial bark\"\n",
    "            ],\n",
    "            \"Excited\": [\n",
    "                \"high-pitched rapid excited yips\",\n",
    "                \"quick playful bark\",\n",
    "                \"series of lively yips\",\n",
    "                \"fast bright excited bark\"\n",
    "            ],\n",
    "            \"Demand\": [\n",
    "                \"steady attention-seeking bark\",\n",
    "                \"regular rhythmic demand bark\",\n",
    "                \"persistent repetitive request bark\",\n",
    "                \"moderate-pitch insistence bark\"\n",
    "            ],\n",
    "            \"Fear\": [\n",
    "                \"high-pitched trembling fearful bark\",\n",
    "                \"quivering anxious bark\",\n",
    "                \"shaky high anxious bark\",\n",
    "                \"piercing nervous bark\"\n",
    "            ],\n",
    "            \"Aggressive\": [\n",
    "                \"low guttural aggressive bark\",\n",
    "                \"deep harsh threat bark\",\n",
    "                \"rough menacing bark\",\n",
    "                \"raspy growling attack bark\"\n",
    "            ],\n",
    "            \"Pain\": [\n",
    "                \"single sharp pain yelp\",\n",
    "                \"shrill acute pain bark\",\n",
    "                \"sudden high pain yelp\",\n",
    "                \"short piercing pain bark\"\n",
    "            ],\n",
    "            \"Lonely\": [\n",
    "                \"slow spaced lonely bark\",\n",
    "                \"mournful drawn-out bark\",\n",
    "                \"distant monotone lonely bark\",\n",
    "                \"long-interval melancholy bark\"\n",
    "            ],\n",
    "            \"Howl\": [\n",
    "                \"long plaintive canine howl\",\n",
    "                \"careless, spontaneous howl\",\n",
    "                \"sustained mournful howl\",\n",
    "                \"extended melodic dog howl\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # 预处理文本嵌入\n",
    "        all_prompts, prompt_cls = [], []\n",
    "        for cls, plist in self.audio_prompts.items():\n",
    "            all_prompts.extend(plist)\n",
    "            prompt_cls.extend([cls] * len(plist))\n",
    "        \n",
    "        self.prompt_cls = np.array(prompt_cls)\n",
    "        txt_inputs = self.clap_processor(text=all_prompts, return_tensors=\"pt\", padding=True)\n",
    "        txt_inputs = {k: v.to(self.device) for k, v in txt_inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.text_emb = torch.nn.functional.normalize(\n",
    "                self.clap_model.get_text_features(**txt_inputs), dim=-1\n",
    "            )\n",
    "        \n",
    "    def _init_resnet_model(self):\n",
    "        \"\"\"初始化ResNet18模型用于情绪识别\"\"\"\n",
    "        print(\"[INFO] Loading ResNet18 model...\")\n",
    "        self.resnet_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # 先尝试加载预训练权重以确定类别数量\n",
    "        if os.path.exists('dog_emotion.pth'):\n",
    "            try:\n",
    "                # 加载权重字典\n",
    "                checkpoint = torch.load('dog_emotion.pth', map_location=self.device)\n",
    "                \n",
    "                # 从fc层权重推断类别数量\n",
    "                if 'fc.weight' in checkpoint:\n",
    "                    num_classes = checkpoint['fc.weight'].shape[0]\n",
    "                    print(f\"[INFO] Detected {num_classes} emotion classes from checkpoint\")\n",
    "                else:\n",
    "                    num_classes = 4  # 默认值\n",
    "                    print(f\"[INFO] Using default {num_classes} emotion classes\")\n",
    "                \n",
    "                # 根据实际数据集调整类别名称\n",
    "                if num_classes == 4:\n",
    "                    emotion_classes = ['happy', 'sad', 'angry', 'calm']\n",
    "                elif num_classes == 5:\n",
    "                    emotion_classes = ['happy', 'sad', 'angry', 'calm', 'excited']\n",
    "                else:\n",
    "                    emotion_classes = [f'emotion_{i}' for i in range(num_classes)]\n",
    "                \n",
    "                self.emotion_classes = emotion_classes\n",
    "                self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, num_classes)\n",
    "                \n",
    "                # 加载权重\n",
    "                self.resnet_model.load_state_dict(checkpoint)\n",
    "                print(\"[INFO] Loaded pre-trained emotion model\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Error loading emotion model: {e}\")\n",
    "                print(\"[INFO] Using default emotion classes\")\n",
    "                emotion_classes = ['happy', 'sad', 'angry', 'calm']\n",
    "                self.emotion_classes = emotion_classes\n",
    "                self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, len(emotion_classes))\n",
    "        else:\n",
    "            print(\"[INFO] No pre-trained emotion model found, using default classes\")\n",
    "            emotion_classes = ['happy', 'sad', 'angry', 'calm']\n",
    "            self.emotion_classes = emotion_classes\n",
    "            self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, len(emotion_classes))\n",
    "        \n",
    "        self.resnet_model = self.resnet_model.to(self.device)\n",
    "        self.resnet_model.eval()\n",
    "        \n",
    "        # 图像预处理\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def _init_llm_model(self):\n",
    "        \"\"\"初始化LLM模型用于最终分析\"\"\"\n",
    "        print(\"[INFO] Loading LLM model...\")\n",
    "        try:\n",
    "            self.llm = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=\"microsoft/DialoGPT-medium\",\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "        except:\n",
    "            print(\"[WARNING] Could not load LLM model, will use rule-based analysis\")\n",
    "            self.llm = None\n",
    "    \n",
    "    def extract_video_components(self, video_path: str, sample_rate: int = 48000) -> Tuple[List[np.ndarray], np.ndarray]:\n",
    "        \"\"\"从视频中提取图片帧和音频\"\"\"\n",
    "        print(f\"[INFO] Extracting components from {video_path}\")\n",
    "        \n",
    "        # 提取视频帧\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        frame_count = 0\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        # 每秒采样1帧\n",
    "        frame_interval = int(fps) if fps > 0 else 30\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            if frame_count % frame_interval == 0:\n",
    "                # 转换为RGB格式\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame_rgb)\n",
    "                \n",
    "            frame_count += 1\n",
    "            \n",
    "        cap.release()\n",
    "        print(f\"[INFO] Extracted {len(frames)} frames\")\n",
    "        \n",
    "        # 提取音频\n",
    "        try:\n",
    "            audio, sr = librosa.load(video_path, sr=sample_rate, mono=True)\n",
    "            print(f\"[INFO] Extracted audio: {len(audio)} samples at {sr} Hz\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not extract audio: {e}\")\n",
    "            audio = np.array([])\n",
    "            \n",
    "        return frames, audio\n",
    "    \n",
    "    def analyze_scenes(self, frames: List[np.ndarray]) -> List[str]:\n",
    "        \"\"\"使用BLIP分析场景\"\"\"\n",
    "        print(\"[INFO] Analyzing scenes with BLIP...\")\n",
    "        scene_descriptions = []\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            try:\n",
    "                # 转换为PIL Image\n",
    "                img = Image.fromarray(frame).convert(\"RGB\")\n",
    "                \n",
    "                # BLIP处理\n",
    "                inputs = self.blip_processor(images=img, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.blip_model.device) for k, v in inputs.items()}\n",
    "                \n",
    "                generated_ids = self.blip_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=80,\n",
    "                    num_beams=5,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                )\n",
    "                \n",
    "                description = self.blip_processor.batch_decode(\n",
    "                    generated_ids, skip_special_tokens=True\n",
    "                )[0]\n",
    "                \n",
    "                scene_descriptions.append(description)\n",
    "                print(f\"[INFO] Frame {i+1}: {description}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Error analyzing frame {i+1}: {e}\")\n",
    "                scene_descriptions.append(\"Unable to analyze scene\")\n",
    "                \n",
    "        return scene_descriptions\n",
    "    \n",
    "    def analyze_emotions(self, frames: List[np.ndarray]) -> List[Dict]:\n",
    "        \"\"\"使用ResNet18分析情绪\"\"\"\n",
    "        print(\"[INFO] Analyzing emotions with ResNet18...\")\n",
    "        emotion_results = []\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            try:\n",
    "                # 转换为PIL Image并预处理\n",
    "                img = Image.fromarray(frame).convert(\"RGB\")\n",
    "                img_tensor = self.image_transform(img).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.resnet_model(img_tensor)\n",
    "                    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "                    \n",
    "                    # 获取预测结果\n",
    "                    pred_idx = torch.argmax(probabilities).item()\n",
    "                    confidence = probabilities[pred_idx].item()\n",
    "                    \n",
    "                    emotion_result = {\n",
    "                        'emotion': self.emotion_classes[pred_idx],\n",
    "                        'confidence': confidence,\n",
    "                        'all_probabilities': {\n",
    "                            emotion: prob.item() \n",
    "                            for emotion, prob in zip(self.emotion_classes, probabilities)\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    emotion_results.append(emotion_result)\n",
    "                    print(f\"[INFO] Frame {i+1}: {emotion_result['emotion']} ({confidence:.3f})\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Error analyzing emotion for frame {i+1}: {e}\")\n",
    "                emotion_results.append({\n",
    "                    'emotion': 'unknown',\n",
    "                    'confidence': 0.0,\n",
    "                    'all_probabilities': {}\n",
    "                })\n",
    "                \n",
    "        return emotion_results\n",
    "    \n",
    "    def analyze_audio(self, audio: np.ndarray, sample_rate: int = 48000) -> Dict:\n",
    "        \"\"\"使用CLAP分析音频\"\"\"\n",
    "        print(\"[INFO] Analyzing audio with CLAP...\")\n",
    "        \n",
    "        if len(audio) == 0:\n",
    "            return {\n",
    "                'predicted_class': 'Silent',\n",
    "                'confidence': 0.0,\n",
    "                'all_probabilities': {}\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # 分段处理音频\n",
    "            seg_seconds = 10\n",
    "            seg_len = seg_seconds * sample_rate\n",
    "            segments = [audio[i:i+seg_len] for i in range(0, len(audio), seg_len)]\n",
    "            seg_sims = []\n",
    "            \n",
    "            for seg in segments:\n",
    "                if len(seg) < 1000:  # 跳过过短片段\n",
    "                    continue\n",
    "                    \n",
    "                a_in = self.clap_processor(\n",
    "                    audios=[seg], \n",
    "                    sampling_rate=sample_rate,\n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True\n",
    "                )\n",
    "                a_in = {k: v.to(self.device) for k, v in a_in.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    a_emb = torch.nn.functional.normalize(\n",
    "                        self.clap_model.get_audio_features(**a_in), dim=-1\n",
    "                    )\n",
    "                    \n",
    "                sim = (a_emb @ self.text_emb.T).softmax(dim=-1)[0].cpu().numpy()\n",
    "                seg_sims.append(sim)\n",
    "            \n",
    "            if not seg_sims:\n",
    "                seg_sims = [np.zeros(len(self.text_emb))]\n",
    "            \n",
    "            sim_avg = np.mean(np.vstack(seg_sims), axis=0)\n",
    "            \n",
    "            # 聚合到类别\n",
    "            cls_prob = {}\n",
    "            for cls in self.audio_prompts:\n",
    "                cls_prob[cls] = float(np.mean(sim_avg[self.prompt_cls == cls]))\n",
    "            \n",
    "            # 预测类别\n",
    "            pred_cls = max(cls_prob, key=cls_prob.get)\n",
    "            confidence = cls_prob[pred_cls]\n",
    "            \n",
    "            print(f\"[INFO] Audio analysis: {pred_cls} ({confidence:.3f})\")\n",
    "            \n",
    "            return {\n",
    "                'predicted_class': pred_cls,\n",
    "                'confidence': confidence,\n",
    "                'all_probabilities': cls_prob\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Error analyzing audio: {e}\")\n",
    "            return {\n",
    "                'predicted_class': 'Unknown',\n",
    "                'confidence': 0.0,\n",
    "                'all_probabilities': {}\n",
    "            }\n",
    "    \n",
    "    def synthesize_analysis(self, scene_descriptions: List[str], \n",
    "                          emotion_results: List[Dict], \n",
    "                          audio_result: Dict) -> Dict:\n",
    "        \"\"\"综合分析结果\"\"\"\n",
    "        print(\"[INFO] Synthesizing analysis...\")\n",
    "        \n",
    "        # 统计最常见的情绪\n",
    "        emotions = [result['emotion'] for result in emotion_results if result['emotion'] != 'unknown']\n",
    "        most_common_emotion = max(set(emotions), key=emotions.count) if emotions else 'unknown'\n",
    "        \n",
    "        # 平均情绪置信度\n",
    "        avg_emotion_confidence = np.mean([\n",
    "            result['confidence'] for result in emotion_results \n",
    "            if result['confidence'] > 0\n",
    "        ]) if emotion_results else 0.0\n",
    "        \n",
    "        # 场景关键词提取\n",
    "        scene_keywords = []\n",
    "        for desc in scene_descriptions:\n",
    "            # 简单的关键词提取\n",
    "            keywords = [word.lower() for word in desc.split() \n",
    "                       if len(word) > 3 and word.lower() not in ['the', 'and', 'with', 'that', 'this']]\n",
    "            scene_keywords.extend(keywords)\n",
    "        \n",
    "        # 生成综合分析\n",
    "        analysis = {\n",
    "            'dominant_emotion': most_common_emotion,\n",
    "            'emotion_confidence': avg_emotion_confidence,\n",
    "            'audio_behavior': audio_result['predicted_class'],\n",
    "            'audio_confidence': audio_result['confidence'],\n",
    "            'scene_context': scene_descriptions,\n",
    "            'scene_keywords': list(set(scene_keywords)),\n",
    "            'detailed_emotions': emotion_results,\n",
    "            'audio_details': audio_result\n",
    "        }\n",
    "        \n",
    "        # 生成狗的想法和心情推测\n",
    "        dog_analysis = self._generate_dog_thoughts(analysis)\n",
    "        analysis['dog_thoughts'] = dog_analysis\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _generate_dog_thoughts(self, analysis: Dict) -> Dict:\n",
    "        \"\"\"基于分析结果推测狗的想法和心情\"\"\"\n",
    "        \n",
    "        emotion = analysis['dominant_emotion']\n",
    "        audio_behavior = analysis['audio_behavior']\n",
    "        scene_keywords = analysis['scene_keywords']\n",
    "        \n",
    "        # 规则基础的推测\n",
    "        thoughts = {\n",
    "            'mood': 'neutral',\n",
    "            'likely_thoughts': [],\n",
    "            'behavioral_interpretation': '',\n",
    "            'needs_attention': False\n",
    "        }\n",
    "        \n",
    "        # 基于情绪和音频行为的推测\n",
    "        if emotion == 'happy' and audio_behavior in ['Excited', 'Demand']:\n",
    "            thoughts['mood'] = 'playful'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I want to play!',\n",
    "                'This is fun!',\n",
    "                'Pay attention to me!'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog appears to be in a playful mood and seeking interaction.'\n",
    "            \n",
    "        elif emotion == 'sad' or audio_behavior in ['Lonely', 'Pain']:\n",
    "            thoughts['mood'] = 'distressed'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I feel lonely',\n",
    "                'I need comfort',\n",
    "                'Something is bothering me'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog may be experiencing distress and needs comfort or attention.'\n",
    "            thoughts['needs_attention'] = True\n",
    "            \n",
    "        elif audio_behavior in ['Alert', 'Territorial']:\n",
    "            thoughts['mood'] = 'vigilant'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'Something is happening',\n",
    "                'I need to protect my territory',\n",
    "                'Alert! Someone is coming'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog is in alert mode, possibly responding to external stimuli.'\n",
    "            \n",
    "        elif emotion == 'angry' or audio_behavior == 'Aggressive':\n",
    "            thoughts['mood'] = 'defensive'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I feel threatened',\n",
    "                'Stay away from me',\n",
    "                'I need to defend myself'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog is displaying defensive or aggressive behavior.'\n",
    "            thoughts['needs_attention'] = True\n",
    "            \n",
    "        elif emotion == 'calm':\n",
    "            thoughts['mood'] = 'relaxed'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I feel comfortable',\n",
    "                'Everything is peaceful',\n",
    "                'I am content'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog appears to be in a relaxed and comfortable state.'\n",
    "            \n",
    "        # 处理未知情绪或其他情绪类别\n",
    "        elif emotion and emotion not in ['unknown', 'neutral']:\n",
    "            thoughts['mood'] = emotion\n",
    "            thoughts['likely_thoughts'] = [f'I am feeling {emotion}']\n",
    "            thoughts['behavioral_interpretation'] = f'The dog is displaying {emotion} behavior.'\n",
    "            \n",
    "        else:\n",
    "            thoughts['mood'] = 'neutral'\n",
    "            thoughts['likely_thoughts'] = ['Observing the environment']\n",
    "            thoughts['behavioral_interpretation'] = 'The dog is in a neutral state, observing its surroundings.'\n",
    "        \n",
    "        # 基于场景关键词的额外推测\n",
    "        if 'food' in scene_keywords or 'eating' in scene_keywords:\n",
    "            thoughts['likely_thoughts'].append('Food! I want some!')\n",
    "            \n",
    "        if 'person' in scene_keywords or 'human' in scene_keywords:\n",
    "            thoughts['likely_thoughts'].append('My human is here!')\n",
    "            \n",
    "        if 'outside' in scene_keywords or 'park' in scene_keywords:\n",
    "            thoughts['likely_thoughts'].append('Time for adventure!')\n",
    "        \n",
    "        return thoughts\n",
    "    \n",
    "    def analyze_video(self, video_path: str) -> Dict:\n",
    "        \"\"\"主函数：分析视频\"\"\"\n",
    "        print(f\"[INFO] Starting video analysis for: {video_path}\")\n",
    "        \n",
    "        # 1. 提取视频组件\n",
    "        frames, audio = self.extract_video_components(video_path)\n",
    "        \n",
    "        # 2. 分析场景\n",
    "        scene_descriptions = self.analyze_scenes(frames)\n",
    "        \n",
    "        # 3. 分析情绪\n",
    "        emotion_results = self.analyze_emotions(frames)\n",
    "        \n",
    "        # 4. 分析音频\n",
    "        audio_result = self.analyze_audio(audio)\n",
    "        \n",
    "        # 5. 综合分析\n",
    "        final_analysis = self.synthesize_analysis(\n",
    "            scene_descriptions, emotion_results, audio_result\n",
    "        )\n",
    "        \n",
    "        print(\"[INFO] Video analysis completed!\")\n",
    "        return final_analysis\n",
    "    \n",
    "    def save_results(self, results: Dict, output_path: str):\n",
    "        \"\"\"保存分析结果\"\"\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[INFO] Results saved to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"主程序\"\"\"\n",
    "    # 初始化分析系统\n",
    "    analyzer = VideoAnalysisSystem()\n",
    "    \n",
    "    # 分析视频\n",
    "    video_path = \"dog.mp4\"  # 替换为您的视频路径\n",
    "    \n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"[ERROR] Video file not found: {video_path}\")\n",
    "        print(\"[INFO] Please make sure the video file exists or update the video_path variable\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # 执行分析\n",
    "        results = analyzer.analyze_video(video_path)\n",
    "        \n",
    "        # 打印结果摘要\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"🐕 DOG ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Mood: {results['dog_thoughts']['mood']}\")\n",
    "        print(f\"Dominant Emotion: {results['dominant_emotion']}\")\n",
    "        print(f\"Audio Behavior: {results['audio_behavior']}\")\n",
    "        print(f\"Needs Attention: {results['dog_thoughts']['needs_attention']}\")\n",
    "        print(f\"Behavioral Interpretation: {results['dog_thoughts']['behavioral_interpretation']}\")\n",
    "        print(\"\\nLikely Thoughts:\")\n",
    "        for thought in results['dog_thoughts']['likely_thoughts']:\n",
    "            print(f\"  - {thought}\")\n",
    "        \n",
    "        # 保存完整结果\n",
    "        analyzer.save_results(results, \"dog_analysis_results.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred during analysis: {e}\")\n",
    "        print(\"[INFO] Please check your video file and model files\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7899561-eab0-4d91-ac6e-48146ef718fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Loading BLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [02:17<00:00, 68.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading CLAP model...\n",
      "[INFO] Loading ResNet18 model...\n",
      "[INFO] Detected 4 emotion classes from checkpoint\n",
      "[INFO] Loaded pre-trained emotion model\n",
      "[INFO] Loading LLM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting video analysis for: dog.mp4\n",
      "[INFO] Extracting components from dog.mp4\n",
      "[INFO] Extracted 3 frames\n",
      "[WARNING] Could not extract audio: \n",
      "[INFO] Analyzing scenes with BLIP...\n",
      "[INFO] Frame 1:  and\n",
      "[INFO] Frame 2:  and\n",
      "[INFO] Frame 3:  and\n",
      "[INFO] Analyzing emotions with ResNet18...\n",
      "[INFO] Frame 1: happy (0.855)\n",
      "[INFO] Frame 2: happy (0.816)\n",
      "[INFO] Frame 3: happy (0.976)\n",
      "[INFO] Analyzing audio with CLAP...\n",
      "[INFO] Synthesizing analysis...\n",
      "[INFO] Video analysis completed!\n",
      "\n",
      "==================================================\n",
      "🐕 DOG ANALYSIS SUMMARY\n",
      "==================================================\n",
      "Mood: happy\n",
      "Dominant Emotion: happy\n",
      "Audio Behavior: Silent\n",
      "Needs Attention: False\n",
      "Behavioral Interpretation: The dog is displaying happy behavior.\n",
      "\n",
      "Likely Thoughts:\n",
      "  - I am feeling happy\n",
      "[INFO] Results saved to dog_analysis_results.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig, \n",
    "    Blip2Processor, \n",
    "    Blip2ForConditionalGeneration,\n",
    "    ClapModel, \n",
    "    ClapProcessor,\n",
    "    pipeline\n",
    ")\n",
    "from torchvision import transforms, models\n",
    "from torch import nn\n",
    "import tempfile\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import subprocess\n",
    "\n",
    "class VideoAnalysisSystem:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"[INFO] Using device: {self.device}\")\n",
    "        \n",
    "        # 初始化所有模型\n",
    "        self._init_blip_model()\n",
    "        self._init_clap_model()\n",
    "        self._init_resnet_model()\n",
    "        self._init_llm_model()\n",
    "        \n",
    "    def _init_blip_model(self):\n",
    "        print(\"[INFO] Loading BLIP model...\")\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True,\n",
    "        )\n",
    "        \n",
    "        self.blip_processor = Blip2Processor.from_pretrained(\n",
    "            \"Salesforce/blip2-opt-2.7b\", use_fast=True\n",
    "        )\n",
    "        self.blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/blip2-opt-2.7b\",\n",
    "            quantization_config=quant_config,\n",
    "            device_map=\"auto\",\n",
    "            offload_folder=\"offload\",\n",
    "            offload_state_dict=True\n",
    "        )\n",
    "        self.blip_model.eval()\n",
    "\n",
    "        \n",
    "    def _init_clap_model(self):\n",
    "        print(\"[INFO] Loading CLAP model...\")\n",
    "        self.clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-fused\").to(self.device)\n",
    "        self.clap_processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n",
    "        \n",
    "        self.audio_prompts = {\n",
    "            \"Alert\": [\n",
    "                \"sharp mid-pitch alert bark\",\n",
    "                \"brief clear alarm bark\",\n",
    "                \"short crisp warning bark\",\n",
    "                \"fast staccato alert bark\"\n",
    "            ],\n",
    "            \"Territorial\": [\n",
    "                \"deep sustained territorial bark\",\n",
    "                \"low throaty guard bark\",\n",
    "                \"prolonged defensive bark\",\n",
    "                \"slow booming territorial bark\"\n",
    "            ],\n",
    "            \"Excited\": [\n",
    "                \"high-pitched rapid excited yips\",\n",
    "                \"quick playful bark\",\n",
    "                \"series of lively yips\",\n",
    "                \"fast bright excited bark\"\n",
    "            ],\n",
    "            \"Demand\": [\n",
    "                \"steady attention-seeking bark\",\n",
    "                \"regular rhythmic demand bark\",\n",
    "                \"persistent repetitive request bark\",\n",
    "                \"moderate-pitch insistence bark\"\n",
    "            ],\n",
    "            \"Fear\": [\n",
    "                \"high-pitched trembling fearful bark\",\n",
    "                \"quivering anxious bark\",\n",
    "                \"shaky high anxious bark\",\n",
    "                \"piercing nervous bark\"\n",
    "            ],\n",
    "            \"Aggressive\": [\n",
    "                \"low guttural aggressive bark\",\n",
    "                \"deep harsh threat bark\",\n",
    "                \"rough menacing bark\",\n",
    "                \"raspy growling attack bark\"\n",
    "            ],\n",
    "            \"Pain\": [\n",
    "                \"single sharp pain yelp\",\n",
    "                \"shrill acute pain bark\",\n",
    "                \"sudden high pain yelp\",\n",
    "                \"short piercing pain bark\"\n",
    "            ],\n",
    "            \"Lonely\": [\n",
    "                \"slow spaced lonely bark\",\n",
    "                \"mournful drawn-out bark\",\n",
    "                \"distant monotone lonely bark\",\n",
    "                \"long-interval melancholy bark\"\n",
    "            ],\n",
    "            \"Howl\": [\n",
    "                \"long plaintive canine howl\",\n",
    "                \"careless, spontaneous howl\",\n",
    "                \"sustained mournful howl\",\n",
    "                \"extended melodic dog howl\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        all_prompts, prompt_cls = [], []\n",
    "        for cls, plist in self.audio_prompts.items():\n",
    "            all_prompts.extend(plist)\n",
    "            prompt_cls.extend([cls] * len(plist))\n",
    "        \n",
    "        self.prompt_cls = np.array(prompt_cls)\n",
    "        txt_inputs = self.clap_processor(text=all_prompts, return_tensors=\"pt\", padding=True)\n",
    "        txt_inputs = {k: v.to(self.device) for k, v in txt_inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.text_emb = torch.nn.functional.normalize(\n",
    "                self.clap_model.get_text_features(**txt_inputs), dim=-1\n",
    "            )\n",
    "        \n",
    "    def _init_resnet_model(self):\n",
    "        print(\"[INFO] Loading ResNet18 model...\")\n",
    "        self.resnet_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        if os.path.exists('dog_emotion.pth'):\n",
    "            try:\n",
    "                checkpoint = torch.load('dog_emotion.pth', map_location=self.device)\n",
    "                \n",
    "                if 'fc.weight' in checkpoint:\n",
    "                    num_classes = checkpoint['fc.weight'].shape[0]\n",
    "                    print(f\"[INFO] Detected {num_classes} emotion classes from checkpoint\")\n",
    "                else:\n",
    "                    num_classes = 4\n",
    "                    print(f\"[INFO] Using default {num_classes} emotion classes\")\n",
    "                \n",
    "                if num_classes == 4:\n",
    "                    emotion_classes = ['angry', 'happy', 'relaxed', 'sad']\n",
    "                elif num_classes == 5:\n",
    "                    emotion_classes = ['angry', 'happy', 'relaxed', 'sad', 'calm']\n",
    "                else:\n",
    "                    emotion_classes = [f'emotion_{i}' for i in range(num_classes)]\n",
    "                \n",
    "                self.emotion_classes = emotion_classes\n",
    "                self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, num_classes)\n",
    "                \n",
    "                self.resnet_model.load_state_dict(checkpoint)\n",
    "                print(\"[INFO] Loaded pre-trained emotion model\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Error loading emotion model: {e}\")\n",
    "                print(\"[INFO] Using default emotion classes\")\n",
    "                emotion_classes = ['angry', 'happy', 'relaxed', 'sad']\n",
    "                self.emotion_classes = emotion_classes\n",
    "                self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, len(emotion_classes))\n",
    "        else:\n",
    "            print(\"[INFO] No pre-trained emotion model found, using default classes\")\n",
    "            emotion_classes = ['angry', 'happy', 'relaxed', 'sad']\n",
    "            self.emotion_classes = emotion_classes\n",
    "            self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, len(emotion_classes))\n",
    "        \n",
    "        self.resnet_model = self.resnet_model.to(self.device)\n",
    "        self.resnet_model.eval()\n",
    "        \n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def _init_llm_model(self):\n",
    "        print(\"[INFO] Loading LLM model...\")\n",
    "        try:\n",
    "            self.llm = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=\"microsoft/DialoGPT-medium\",\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "        except:\n",
    "            print(\"[WARNING] Could not load LLM model, will use rule-based analysis\")\n",
    "            self.llm = None\n",
    "    \n",
    "    def extract_video_components(self, video_path: str, sample_rate: int = 48000) -> Tuple[List[np.ndarray], np.ndarray]:\n",
    "        print(f\"[INFO] Extracting components from {video_path}\")\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames <= 0:\n",
    "            print(\"[WARNING] Could not get total frame count, reading sequentially.\")\n",
    "            for _ in range(3):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame_rgb)\n",
    "        else:\n",
    "            indices = [0, total_frames // 2, total_frames - 1]\n",
    "            unique_indices = sorted(set(indices))\n",
    "            for idx in unique_indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"[WARNING] Could not read frame at index {idx}\")\n",
    "                    continue\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame_rgb)\n",
    "        \n",
    "        cap.release()\n",
    "        print(f\"[INFO] Extracted {len(frames)} frames\")\n",
    "        \n",
    "        try:\n",
    "            audio, sr = librosa.load(video_path, sr=sample_rate, mono=True)\n",
    "            print(f\"[INFO] Extracted audio: {len(audio)} samples at {sr} Hz\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not extract audio: {e}\")\n",
    "            audio = np.array([])\n",
    "        \n",
    "        return frames, audio\n",
    "\n",
    "    \n",
    "    def analyze_scenes(self, frames: List[np.ndarray]) -> List[str]:\n",
    "        print(\"[INFO] Analyzing scenes with BLIP...\")\n",
    "        scene_descriptions = []\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            try:\n",
    "                img = Image.fromarray(frame).convert(\"RGB\")\n",
    "                \n",
    "                inputs = self.blip_processor(images=img, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.blip_model.device) for k, v in inputs.items()}\n",
    "                \n",
    "                generated_ids = self.blip_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=80,\n",
    "                    num_beams=5,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                )\n",
    "                \n",
    "                description = self.blip_processor.batch_decode(\n",
    "                    generated_ids, skip_special_tokens=True\n",
    "                )[0]\n",
    "                \n",
    "                scene_descriptions.append(description)\n",
    "                print(f\"[INFO] Frame {i+1}: {description}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Error analyzing frame {i+1}: {e}\")\n",
    "                scene_descriptions.append(\"Unable to analyze scene\")\n",
    "                \n",
    "        return scene_descriptions\n",
    "    \n",
    "    def analyze_emotions(self, frames: List[np.ndarray]) -> List[Dict]:\n",
    "        print(\"[INFO] Analyzing emotions with ResNet18...\")\n",
    "        emotion_results = []\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            try:\n",
    "                # 转换为PIL Image并预处理\n",
    "                img = Image.fromarray(frame).convert(\"RGB\")\n",
    "                img_tensor = self.image_transform(img).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.resnet_model(img_tensor)\n",
    "                    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "                    \n",
    "                    # 获取预测结果\n",
    "                    pred_idx = torch.argmax(probabilities).item()\n",
    "                    confidence = probabilities[pred_idx].item()\n",
    "                    \n",
    "                    emotion_result = {\n",
    "                        'emotion': self.emotion_classes[pred_idx],\n",
    "                        'confidence': confidence,\n",
    "                        'all_probabilities': {\n",
    "                            emotion: prob.item() \n",
    "                            for emotion, prob in zip(self.emotion_classes, probabilities)\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    emotion_results.append(emotion_result)\n",
    "                    print(f\"[INFO] Frame {i+1}: {emotion_result['emotion']} ({confidence:.3f})\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Error analyzing emotion for frame {i+1}: {e}\")\n",
    "                emotion_results.append({\n",
    "                    'emotion': 'unknown',\n",
    "                    'confidence': 0.0,\n",
    "                    'all_probabilities': {}\n",
    "                })\n",
    "                \n",
    "        return emotion_results\n",
    "    \n",
    "    def analyze_audio(self, audio: np.ndarray, sample_rate: int = 48000) -> Dict:\n",
    "        print(\"[INFO] Analyzing audio with CLAP...\")\n",
    "        \n",
    "        if len(audio) == 0:\n",
    "            return {\n",
    "                'predicted_class': 'Silent',\n",
    "                'confidence': 0.0,\n",
    "                'all_probabilities': {}\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # 分段处理音频\n",
    "            seg_seconds = 10\n",
    "            seg_len = seg_seconds * sample_rate\n",
    "            segments = [audio[i:i+seg_len] for i in range(0, len(audio), seg_len)]\n",
    "            seg_sims = []\n",
    "            \n",
    "            for seg in segments:\n",
    "                if len(seg) < 1000:\n",
    "                    continue\n",
    "                    \n",
    "                a_in = self.clap_processor(\n",
    "                    audios=[seg], \n",
    "                    sampling_rate=sample_rate,\n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True\n",
    "                )\n",
    "                a_in = {k: v.to(self.device) for k, v in a_in.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    a_emb = torch.nn.functional.normalize(\n",
    "                        self.clap_model.get_audio_features(**a_in), dim=-1\n",
    "                    )\n",
    "                    \n",
    "                sim = (a_emb @ self.text_emb.T).softmax(dim=-1)[0].cpu().numpy()\n",
    "                seg_sims.append(sim)\n",
    "            \n",
    "            if not seg_sims:\n",
    "                seg_sims = [np.zeros(len(self.text_emb))]\n",
    "            \n",
    "            sim_avg = np.mean(np.vstack(seg_sims), axis=0)\n",
    "            \n",
    "            cls_prob = {}\n",
    "            for cls in self.audio_prompts:\n",
    "                cls_prob[cls] = float(np.mean(sim_avg[self.prompt_cls == cls]))\n",
    "            \n",
    "            pred_cls = max(cls_prob, key=cls_prob.get)\n",
    "            confidence = cls_prob[pred_cls]\n",
    "            \n",
    "            print(f\"[INFO] Audio analysis: {pred_cls} ({confidence:.3f})\")\n",
    "            \n",
    "            return {\n",
    "                'predicted_class': pred_cls,\n",
    "                'confidence': confidence,\n",
    "                'all_probabilities': cls_prob\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Error analyzing audio: {e}\")\n",
    "            return {\n",
    "                'predicted_class': 'Unknown',\n",
    "                'confidence': 0.0,\n",
    "                'all_probabilities': {}\n",
    "            }\n",
    "    \n",
    "    def synthesize_analysis(self, scene_descriptions: List[str], \n",
    "                          emotion_results: List[Dict], \n",
    "                          audio_result: Dict) -> Dict:\n",
    "        print(\"[INFO] Synthesizing analysis...\")\n",
    "        \n",
    "        emotions = [result['emotion'] for result in emotion_results if result['emotion'] != 'unknown']\n",
    "        most_common_emotion = max(set(emotions), key=emotions.count) if emotions else 'unknown'\n",
    "        \n",
    "        avg_emotion_confidence = np.mean([\n",
    "            result['confidence'] for result in emotion_results \n",
    "            if result['confidence'] > 0\n",
    "        ]) if emotion_results else 0.0\n",
    "        \n",
    "        scene_keywords = []\n",
    "        for desc in scene_descriptions:\n",
    "            keywords = [word.lower() for word in desc.split() \n",
    "                       if len(word) > 3 and word.lower() not in ['the', 'and', 'with', 'that', 'this']]\n",
    "            scene_keywords.extend(keywords)\n",
    "        \n",
    "        analysis = {\n",
    "            'dominant_emotion': most_common_emotion,\n",
    "            'emotion_confidence': avg_emotion_confidence,\n",
    "            'audio_behavior': audio_result['predicted_class'],\n",
    "            'audio_confidence': audio_result['confidence'],\n",
    "            'scene_context': scene_descriptions,\n",
    "            'scene_keywords': list(set(scene_keywords)),\n",
    "            'detailed_emotions': emotion_results,\n",
    "            'audio_details': audio_result\n",
    "        }\n",
    "        \n",
    "        dog_analysis = self._generate_dog_thoughts(analysis)\n",
    "        analysis['dog_thoughts'] = dog_analysis\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _generate_dog_thoughts(self, analysis: Dict) -> Dict:\n",
    "        emotion = analysis['dominant_emotion']\n",
    "        audio_behavior = analysis['audio_behavior']\n",
    "        scene_keywords = analysis['scene_keywords']\n",
    "        \n",
    "        thoughts = {\n",
    "            'mood': 'neutral',\n",
    "            'likely_thoughts': [],\n",
    "            'behavioral_interpretation': '',\n",
    "            'needs_attention': False\n",
    "        }\n",
    "        \n",
    "        if emotion == 'happy' and audio_behavior in ['Excited', 'Demand']:\n",
    "            thoughts['mood'] = 'playful'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I want to play!',\n",
    "                'This is fun!',\n",
    "                'Pay attention to me!'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog appears to be in a playful mood and seeking interaction.'\n",
    "            \n",
    "        elif emotion == 'sad' or audio_behavior in ['Lonely', 'Pain']:\n",
    "            thoughts['mood'] = 'distressed'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I feel lonely',\n",
    "                'I need comfort',\n",
    "                'Something is bothering me'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog may be experiencing distress and needs comfort or attention.'\n",
    "            thoughts['needs_attention'] = True\n",
    "            \n",
    "        elif audio_behavior in ['Alert', 'Territorial']:\n",
    "            thoughts['mood'] = 'vigilant'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'Something is happening',\n",
    "                'I need to protect my territory',\n",
    "                'Alert! Someone is coming'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog is in alert mode, possibly responding to external stimuli.'\n",
    "            \n",
    "        elif emotion == 'angry' or audio_behavior == 'Aggressive':\n",
    "            thoughts['mood'] = 'defensive'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I feel threatened',\n",
    "                'Stay away from me',\n",
    "                'I need to defend myself'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog is displaying defensive or aggressive behavior.'\n",
    "            thoughts['needs_attention'] = True\n",
    "            \n",
    "        elif emotion == 'calm':\n",
    "            thoughts['mood'] = 'relaxed'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I feel comfortable',\n",
    "                'Everything is peaceful',\n",
    "                'I am content'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog appears to be in a relaxed and comfortable state.'\n",
    "            \n",
    "        elif emotion and emotion not in ['unknown', 'neutral']:\n",
    "            thoughts['mood'] = emotion\n",
    "            thoughts['likely_thoughts'] = [f'I am feeling {emotion}']\n",
    "            thoughts['behavioral_interpretation'] = f'The dog is displaying {emotion} behavior.'\n",
    "            \n",
    "        else:\n",
    "            thoughts['mood'] = 'neutral'\n",
    "            thoughts['likely_thoughts'] = ['Observing the environment']\n",
    "            thoughts['behavioral_interpretation'] = 'The dog is in a neutral state, observing its surroundings.'\n",
    "        \n",
    "        if 'food' in scene_keywords or 'eating' in scene_keywords:\n",
    "            thoughts['likely_thoughts'].append('Food! I want some!')\n",
    "            \n",
    "        if 'person' in scene_keywords or 'human' in scene_keywords:\n",
    "            thoughts['likely_thoughts'].append('My human is here!')\n",
    "            \n",
    "        if 'outside' in scene_keywords or 'park' in scene_keywords:\n",
    "            thoughts['likely_thoughts'].append('Time for adventure!')\n",
    "        \n",
    "        return thoughts\n",
    "    \n",
    "    def analyze_video(self, video_path: str) -> Dict:\n",
    "        print(f\"[INFO] Starting video analysis for: {video_path}\")\n",
    "        \n",
    "        frames, audio = self.extract_video_components(video_path)\n",
    "        \n",
    "        scene_descriptions = self.analyze_scenes(frames)\n",
    "        \n",
    "        emotion_results = self.analyze_emotions(frames)\n",
    "        \n",
    "        audio_result = self.analyze_audio(audio)\n",
    "        \n",
    "        final_analysis = self.synthesize_analysis(\n",
    "            scene_descriptions, emotion_results, audio_result\n",
    "        )\n",
    "        \n",
    "        print(\"[INFO] Video analysis completed!\")\n",
    "        return final_analysis\n",
    "    \n",
    "    def save_results(self, results: Dict, output_path: str):\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[INFO] Results saved to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    analyzer = VideoAnalysisSystem()\n",
    "\n",
    "    video_path = \"dog.mp4\"\n",
    "    \n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"[ERROR] Video file not found: {video_path}\")\n",
    "        print(\"[INFO] Please make sure the video file exists or update the video_path variable\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        results = analyzer.analyze_video(video_path)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"🐕 DOG ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Mood: {results['dog_thoughts']['mood']}\")\n",
    "        print(f\"Dominant Emotion: {results['dominant_emotion']}\")\n",
    "        print(f\"Audio Behavior: {results['audio_behavior']}\")\n",
    "        print(f\"Needs Attention: {results['dog_thoughts']['needs_attention']}\")\n",
    "        print(f\"Behavioral Interpretation: {results['dog_thoughts']['behavioral_interpretation']}\")\n",
    "        print(\"\\nLikely Thoughts:\")\n",
    "        for thought in results['dog_thoughts']['likely_thoughts']:\n",
    "            print(f\"  - {thought}\")\n",
    "\n",
    "        analyzer.save_results(results, \"dog_analysis_results.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred during analysis: {e}\")\n",
    "        print(\"[INFO] Please check your video file and model files\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c88613-7716-41b0-ad85-b2ccf6df6e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
