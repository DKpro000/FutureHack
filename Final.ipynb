{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0581f836-28dc-4ebe-bda2-974a0c180870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Loading BLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:37<00:00, 18.60s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading CLAP model...\n",
      "[INFO] Loading ResNet18 model...\n",
      "[INFO] Detected 4 emotion classes from checkpoint\n",
      "[INFO] Loaded pre-trained emotion model\n",
      "[INFO] Loading LLM model...\n",
      "WARNING:tensorflow:From C:\\Users\\DKpro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting video analysis for: dog.mp4\n",
      "[INFO] Extracting components from dog.mp4\n",
      "[INFO] Extracted 14 frames\n",
      "[WARNING] Could not extract audio: \n",
      "[INFO] Analyzing scenes with BLIP...\n",
      "[INFO] Frame 1: a person walking with a dog in a park\n",
      "\n",
      "[INFO] Frame 2: a woman is petting a golden retriever in a park\n",
      "\n",
      "[INFO] Frame 3: a person standing next to a dog in a park\n",
      "\n",
      "[INFO] Frame 4: a person walking with a dog in a park\n",
      "\n",
      "[INFO] Frame 5: a woman is playing with a dog in a park\n",
      "\n",
      "[INFO] Frame 6: a golden retriever is playing with its owner in a park\n",
      "\n",
      "[INFO] Frame 7: a person walking with a golden retriever in a park\n",
      "\n",
      "[INFO] Frame 8: a woman is playing with a dog in a park\n",
      "\n",
      "[INFO] Frame 9: a golden retriever is playing with a person in a park\n",
      "\n",
      "[INFO] Frame 10: golden retriever playing with his owner in the park\n",
      "\n",
      "[INFO] Frame 11: a woman is playing with a dog in a park\n",
      "\n",
      "[INFO] Frame 12: a person walking with a dog in a park\n",
      "\n",
      "[INFO] Frame 13: a golden retriever playing with a frisbee in a park\n",
      "\n",
      "[INFO] Frame 14: a person petting a dog in a park\n",
      "\n",
      "[INFO] Analyzing emotions with ResNet18...\n",
      "[INFO] Frame 1: angry (0.725)\n",
      "[INFO] Frame 2: angry (0.880)\n",
      "[INFO] Frame 3: angry (0.516)\n",
      "[INFO] Frame 4: angry (0.983)\n",
      "[INFO] Frame 5: sad (0.659)\n",
      "[INFO] Frame 6: angry (0.846)\n",
      "[INFO] Frame 7: angry (0.958)\n",
      "[INFO] Frame 8: sad (0.597)\n",
      "[INFO] Frame 9: angry (0.767)\n",
      "[INFO] Frame 10: angry (0.947)\n",
      "[INFO] Frame 11: sad (0.640)\n",
      "[INFO] Frame 12: angry (0.790)\n",
      "[INFO] Frame 13: sad (0.900)\n",
      "[INFO] Frame 14: sad (0.603)\n",
      "[INFO] Analyzing audio with CLAP...\n",
      "[INFO] Synthesizing analysis...\n",
      "[INFO] Video analysis completed!\n",
      "\n",
      "==================================================\n",
      "ğŸ• DOG ANALYSIS SUMMARY\n",
      "==================================================\n",
      "Mood: defensive\n",
      "Dominant Emotion: angry\n",
      "Audio Behavior: Silent\n",
      "Needs Attention: True\n",
      "Behavioral Interpretation: The dog is displaying defensive or aggressive behavior.\n",
      "\n",
      "Likely Thoughts:\n",
      "  - I feel threatened\n",
      "  - Stay away from me\n",
      "  - I need to defend myself\n",
      "  - My human is here!\n",
      "  - Time for adventure!\n",
      "[INFO] Results saved to dog_analysis_results.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "è§†é¢‘åˆ†æç»Ÿåˆç³»ç»Ÿ\n",
    "å°†è§†é¢‘åˆ†è§£ä¸ºå›¾ç‰‡å’ŒéŸ³é¢‘ï¼Œä½¿ç”¨BLIPã€CLAPå’ŒResNet18è¿›è¡Œåˆ†æï¼Œ\n",
    "ç„¶åç»Ÿåˆç»“æœæ¨æµ‹ç‹—çš„æƒ³æ³•å’Œå¿ƒæƒ…\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig, \n",
    "    Blip2Processor, \n",
    "    Blip2ForConditionalGeneration,\n",
    "    ClapModel, \n",
    "    ClapProcessor,\n",
    "    pipeline\n",
    ")\n",
    "from torchvision import transforms, models\n",
    "from torch import nn\n",
    "import tempfile\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class VideoAnalysisSystem:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"[INFO] Using device: {self.device}\")\n",
    "        \n",
    "        # åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹\n",
    "        self._init_blip_model()\n",
    "        self._init_clap_model()\n",
    "        self._init_resnet_model()\n",
    "        self._init_llm_model()\n",
    "        \n",
    "    def _init_blip_model(self):\n",
    "        \"\"\"åˆå§‹åŒ–BLIPæ¨¡å‹ç”¨äºåœºæ™¯æè¿°\"\"\"\n",
    "        print(\"[INFO] Loading BLIP model...\")\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True,\n",
    "        )\n",
    "        \n",
    "        self.blip_processor = Blip2Processor.from_pretrained(\n",
    "            \"Salesforce/blip2-opt-2.7b\", use_fast=True\n",
    "        )\n",
    "        self.blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/blip2-opt-2.7b\",\n",
    "            quantization_config=quant_config,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self.blip_model.eval()\n",
    "        \n",
    "    def _init_clap_model(self):\n",
    "        \"\"\"åˆå§‹åŒ–CLAPæ¨¡å‹ç”¨äºéŸ³é¢‘åˆ†æ\"\"\"\n",
    "        print(\"[INFO] Loading CLAP model...\")\n",
    "        self.clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-fused\").to(self.device)\n",
    "        self.clap_processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n",
    "        \n",
    "        # å®šä¹‰éŸ³é¢‘åˆ†æçš„æç¤ºè¯\n",
    "        self.audio_prompts = {\n",
    "            \"Alert\": [\n",
    "                \"sharp mid-pitch alert bark\",\n",
    "                \"brief clear alarm bark\",\n",
    "                \"short crisp warning bark\",\n",
    "                \"fast staccato alert bark\"\n",
    "            ],\n",
    "            \"Territorial\": [\n",
    "                \"deep sustained territorial bark\",\n",
    "                \"low throaty guard bark\",\n",
    "                \"prolonged defensive bark\",\n",
    "                \"slow booming territorial bark\"\n",
    "            ],\n",
    "            \"Excited\": [\n",
    "                \"high-pitched rapid excited yips\",\n",
    "                \"quick playful bark\",\n",
    "                \"series of lively yips\",\n",
    "                \"fast bright excited bark\"\n",
    "            ],\n",
    "            \"Demand\": [\n",
    "                \"steady attention-seeking bark\",\n",
    "                \"regular rhythmic demand bark\",\n",
    "                \"persistent repetitive request bark\",\n",
    "                \"moderate-pitch insistence bark\"\n",
    "            ],\n",
    "            \"Fear\": [\n",
    "                \"high-pitched trembling fearful bark\",\n",
    "                \"quivering anxious bark\",\n",
    "                \"shaky high anxious bark\",\n",
    "                \"piercing nervous bark\"\n",
    "            ],\n",
    "            \"Aggressive\": [\n",
    "                \"low guttural aggressive bark\",\n",
    "                \"deep harsh threat bark\",\n",
    "                \"rough menacing bark\",\n",
    "                \"raspy growling attack bark\"\n",
    "            ],\n",
    "            \"Pain\": [\n",
    "                \"single sharp pain yelp\",\n",
    "                \"shrill acute pain bark\",\n",
    "                \"sudden high pain yelp\",\n",
    "                \"short piercing pain bark\"\n",
    "            ],\n",
    "            \"Lonely\": [\n",
    "                \"slow spaced lonely bark\",\n",
    "                \"mournful drawn-out bark\",\n",
    "                \"distant monotone lonely bark\",\n",
    "                \"long-interval melancholy bark\"\n",
    "            ],\n",
    "            \"Howl\": [\n",
    "                \"long plaintive canine howl\",\n",
    "                \"careless, spontaneous howl\",\n",
    "                \"sustained mournful howl\",\n",
    "                \"extended melodic dog howl\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # é¢„å¤„ç†æ–‡æœ¬åµŒå…¥\n",
    "        all_prompts, prompt_cls = [], []\n",
    "        for cls, plist in self.audio_prompts.items():\n",
    "            all_prompts.extend(plist)\n",
    "            prompt_cls.extend([cls] * len(plist))\n",
    "        \n",
    "        self.prompt_cls = np.array(prompt_cls)\n",
    "        txt_inputs = self.clap_processor(text=all_prompts, return_tensors=\"pt\", padding=True)\n",
    "        txt_inputs = {k: v.to(self.device) for k, v in txt_inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.text_emb = torch.nn.functional.normalize(\n",
    "                self.clap_model.get_text_features(**txt_inputs), dim=-1\n",
    "            )\n",
    "        \n",
    "    def _init_resnet_model(self):\n",
    "        \"\"\"åˆå§‹åŒ–ResNet18æ¨¡å‹ç”¨äºæƒ…ç»ªè¯†åˆ«\"\"\"\n",
    "        print(\"[INFO] Loading ResNet18 model...\")\n",
    "        self.resnet_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # å…ˆå°è¯•åŠ è½½é¢„è®­ç»ƒæƒé‡ä»¥ç¡®å®šç±»åˆ«æ•°é‡\n",
    "        if os.path.exists('dog_emotion.pth'):\n",
    "            try:\n",
    "                # åŠ è½½æƒé‡å­—å…¸\n",
    "                checkpoint = torch.load('dog_emotion.pth', map_location=self.device)\n",
    "                \n",
    "                # ä»fcå±‚æƒé‡æ¨æ–­ç±»åˆ«æ•°é‡\n",
    "                if 'fc.weight' in checkpoint:\n",
    "                    num_classes = checkpoint['fc.weight'].shape[0]\n",
    "                    print(f\"[INFO] Detected {num_classes} emotion classes from checkpoint\")\n",
    "                else:\n",
    "                    num_classes = 4  # é»˜è®¤å€¼\n",
    "                    print(f\"[INFO] Using default {num_classes} emotion classes\")\n",
    "                \n",
    "                # æ ¹æ®å®é™…æ•°æ®é›†è°ƒæ•´ç±»åˆ«åç§°\n",
    "                if num_classes == 4:\n",
    "                    emotion_classes = ['happy', 'sad', 'angry', 'calm']\n",
    "                elif num_classes == 5:\n",
    "                    emotion_classes = ['happy', 'sad', 'angry', 'calm', 'excited']\n",
    "                else:\n",
    "                    emotion_classes = [f'emotion_{i}' for i in range(num_classes)]\n",
    "                \n",
    "                self.emotion_classes = emotion_classes\n",
    "                self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, num_classes)\n",
    "                \n",
    "                # åŠ è½½æƒé‡\n",
    "                self.resnet_model.load_state_dict(checkpoint)\n",
    "                print(\"[INFO] Loaded pre-trained emotion model\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Error loading emotion model: {e}\")\n",
    "                print(\"[INFO] Using default emotion classes\")\n",
    "                emotion_classes = ['happy', 'sad', 'angry', 'calm']\n",
    "                self.emotion_classes = emotion_classes\n",
    "                self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, len(emotion_classes))\n",
    "        else:\n",
    "            print(\"[INFO] No pre-trained emotion model found, using default classes\")\n",
    "            emotion_classes = ['happy', 'sad', 'angry', 'calm']\n",
    "            self.emotion_classes = emotion_classes\n",
    "            self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, len(emotion_classes))\n",
    "        \n",
    "        self.resnet_model = self.resnet_model.to(self.device)\n",
    "        self.resnet_model.eval()\n",
    "        \n",
    "        # å›¾åƒé¢„å¤„ç†\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def _init_llm_model(self):\n",
    "        \"\"\"åˆå§‹åŒ–LLMæ¨¡å‹ç”¨äºæœ€ç»ˆåˆ†æ\"\"\"\n",
    "        print(\"[INFO] Loading LLM model...\")\n",
    "        try:\n",
    "            self.llm = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=\"microsoft/DialoGPT-medium\",\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "        except:\n",
    "            print(\"[WARNING] Could not load LLM model, will use rule-based analysis\")\n",
    "            self.llm = None\n",
    "    \n",
    "    def extract_video_components(self, video_path: str, sample_rate: int = 48000) -> Tuple[List[np.ndarray], np.ndarray]:\n",
    "        \"\"\"ä»è§†é¢‘ä¸­æå–å›¾ç‰‡å¸§å’ŒéŸ³é¢‘\"\"\"\n",
    "        print(f\"[INFO] Extracting components from {video_path}\")\n",
    "        \n",
    "        # æå–è§†é¢‘å¸§\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        frame_count = 0\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        # æ¯ç§’é‡‡æ ·1å¸§\n",
    "        frame_interval = int(fps) if fps > 0 else 30\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            if frame_count % frame_interval == 0:\n",
    "                # è½¬æ¢ä¸ºRGBæ ¼å¼\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame_rgb)\n",
    "                \n",
    "            frame_count += 1\n",
    "            \n",
    "        cap.release()\n",
    "        print(f\"[INFO] Extracted {len(frames)} frames\")\n",
    "        \n",
    "        # æå–éŸ³é¢‘\n",
    "        try:\n",
    "            audio, sr = librosa.load(video_path, sr=sample_rate, mono=True)\n",
    "            print(f\"[INFO] Extracted audio: {len(audio)} samples at {sr} Hz\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not extract audio: {e}\")\n",
    "            audio = np.array([])\n",
    "            \n",
    "        return frames, audio\n",
    "    \n",
    "    def analyze_scenes(self, frames: List[np.ndarray]) -> List[str]:\n",
    "        \"\"\"ä½¿ç”¨BLIPåˆ†æåœºæ™¯\"\"\"\n",
    "        print(\"[INFO] Analyzing scenes with BLIP...\")\n",
    "        scene_descriptions = []\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            try:\n",
    "                # è½¬æ¢ä¸ºPIL Image\n",
    "                img = Image.fromarray(frame).convert(\"RGB\")\n",
    "                \n",
    "                # BLIPå¤„ç†\n",
    "                inputs = self.blip_processor(images=img, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.blip_model.device) for k, v in inputs.items()}\n",
    "                \n",
    "                generated_ids = self.blip_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=80,\n",
    "                    num_beams=5,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                )\n",
    "                \n",
    "                description = self.blip_processor.batch_decode(\n",
    "                    generated_ids, skip_special_tokens=True\n",
    "                )[0]\n",
    "                \n",
    "                scene_descriptions.append(description)\n",
    "                print(f\"[INFO] Frame {i+1}: {description}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Error analyzing frame {i+1}: {e}\")\n",
    "                scene_descriptions.append(\"Unable to analyze scene\")\n",
    "                \n",
    "        return scene_descriptions\n",
    "    \n",
    "    def analyze_emotions(self, frames: List[np.ndarray]) -> List[Dict]:\n",
    "        \"\"\"ä½¿ç”¨ResNet18åˆ†ææƒ…ç»ª\"\"\"\n",
    "        print(\"[INFO] Analyzing emotions with ResNet18...\")\n",
    "        emotion_results = []\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            try:\n",
    "                # è½¬æ¢ä¸ºPIL Imageå¹¶é¢„å¤„ç†\n",
    "                img = Image.fromarray(frame).convert(\"RGB\")\n",
    "                img_tensor = self.image_transform(img).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.resnet_model(img_tensor)\n",
    "                    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "                    \n",
    "                    # è·å–é¢„æµ‹ç»“æœ\n",
    "                    pred_idx = torch.argmax(probabilities).item()\n",
    "                    confidence = probabilities[pred_idx].item()\n",
    "                    \n",
    "                    emotion_result = {\n",
    "                        'emotion': self.emotion_classes[pred_idx],\n",
    "                        'confidence': confidence,\n",
    "                        'all_probabilities': {\n",
    "                            emotion: prob.item() \n",
    "                            for emotion, prob in zip(self.emotion_classes, probabilities)\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    emotion_results.append(emotion_result)\n",
    "                    print(f\"[INFO] Frame {i+1}: {emotion_result['emotion']} ({confidence:.3f})\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Error analyzing emotion for frame {i+1}: {e}\")\n",
    "                emotion_results.append({\n",
    "                    'emotion': 'unknown',\n",
    "                    'confidence': 0.0,\n",
    "                    'all_probabilities': {}\n",
    "                })\n",
    "                \n",
    "        return emotion_results\n",
    "    \n",
    "    def analyze_audio(self, audio: np.ndarray, sample_rate: int = 48000) -> Dict:\n",
    "        \"\"\"ä½¿ç”¨CLAPåˆ†æéŸ³é¢‘\"\"\"\n",
    "        print(\"[INFO] Analyzing audio with CLAP...\")\n",
    "        \n",
    "        if len(audio) == 0:\n",
    "            return {\n",
    "                'predicted_class': 'Silent',\n",
    "                'confidence': 0.0,\n",
    "                'all_probabilities': {}\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # åˆ†æ®µå¤„ç†éŸ³é¢‘\n",
    "            seg_seconds = 10\n",
    "            seg_len = seg_seconds * sample_rate\n",
    "            segments = [audio[i:i+seg_len] for i in range(0, len(audio), seg_len)]\n",
    "            seg_sims = []\n",
    "            \n",
    "            for seg in segments:\n",
    "                if len(seg) < 1000:  # è·³è¿‡è¿‡çŸ­ç‰‡æ®µ\n",
    "                    continue\n",
    "                    \n",
    "                a_in = self.clap_processor(\n",
    "                    audios=[seg], \n",
    "                    sampling_rate=sample_rate,\n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True\n",
    "                )\n",
    "                a_in = {k: v.to(self.device) for k, v in a_in.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    a_emb = torch.nn.functional.normalize(\n",
    "                        self.clap_model.get_audio_features(**a_in), dim=-1\n",
    "                    )\n",
    "                    \n",
    "                sim = (a_emb @ self.text_emb.T).softmax(dim=-1)[0].cpu().numpy()\n",
    "                seg_sims.append(sim)\n",
    "            \n",
    "            if not seg_sims:\n",
    "                seg_sims = [np.zeros(len(self.text_emb))]\n",
    "            \n",
    "            sim_avg = np.mean(np.vstack(seg_sims), axis=0)\n",
    "            \n",
    "            # èšåˆåˆ°ç±»åˆ«\n",
    "            cls_prob = {}\n",
    "            for cls in self.audio_prompts:\n",
    "                cls_prob[cls] = float(np.mean(sim_avg[self.prompt_cls == cls]))\n",
    "            \n",
    "            # é¢„æµ‹ç±»åˆ«\n",
    "            pred_cls = max(cls_prob, key=cls_prob.get)\n",
    "            confidence = cls_prob[pred_cls]\n",
    "            \n",
    "            print(f\"[INFO] Audio analysis: {pred_cls} ({confidence:.3f})\")\n",
    "            \n",
    "            return {\n",
    "                'predicted_class': pred_cls,\n",
    "                'confidence': confidence,\n",
    "                'all_probabilities': cls_prob\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Error analyzing audio: {e}\")\n",
    "            return {\n",
    "                'predicted_class': 'Unknown',\n",
    "                'confidence': 0.0,\n",
    "                'all_probabilities': {}\n",
    "            }\n",
    "    \n",
    "    def synthesize_analysis(self, scene_descriptions: List[str], \n",
    "                          emotion_results: List[Dict], \n",
    "                          audio_result: Dict) -> Dict:\n",
    "        \"\"\"ç»¼åˆåˆ†æç»“æœ\"\"\"\n",
    "        print(\"[INFO] Synthesizing analysis...\")\n",
    "        \n",
    "        # ç»Ÿè®¡æœ€å¸¸è§çš„æƒ…ç»ª\n",
    "        emotions = [result['emotion'] for result in emotion_results if result['emotion'] != 'unknown']\n",
    "        most_common_emotion = max(set(emotions), key=emotions.count) if emotions else 'unknown'\n",
    "        \n",
    "        # å¹³å‡æƒ…ç»ªç½®ä¿¡åº¦\n",
    "        avg_emotion_confidence = np.mean([\n",
    "            result['confidence'] for result in emotion_results \n",
    "            if result['confidence'] > 0\n",
    "        ]) if emotion_results else 0.0\n",
    "        \n",
    "        # åœºæ™¯å…³é”®è¯æå–\n",
    "        scene_keywords = []\n",
    "        for desc in scene_descriptions:\n",
    "            # ç®€å•çš„å…³é”®è¯æå–\n",
    "            keywords = [word.lower() for word in desc.split() \n",
    "                       if len(word) > 3 and word.lower() not in ['the', 'and', 'with', 'that', 'this']]\n",
    "            scene_keywords.extend(keywords)\n",
    "        \n",
    "        # ç”Ÿæˆç»¼åˆåˆ†æ\n",
    "        analysis = {\n",
    "            'dominant_emotion': most_common_emotion,\n",
    "            'emotion_confidence': avg_emotion_confidence,\n",
    "            'audio_behavior': audio_result['predicted_class'],\n",
    "            'audio_confidence': audio_result['confidence'],\n",
    "            'scene_context': scene_descriptions,\n",
    "            'scene_keywords': list(set(scene_keywords)),\n",
    "            'detailed_emotions': emotion_results,\n",
    "            'audio_details': audio_result\n",
    "        }\n",
    "        \n",
    "        # ç”Ÿæˆç‹—çš„æƒ³æ³•å’Œå¿ƒæƒ…æ¨æµ‹\n",
    "        dog_analysis = self._generate_dog_thoughts(analysis)\n",
    "        analysis['dog_thoughts'] = dog_analysis\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _generate_dog_thoughts(self, analysis: Dict) -> Dict:\n",
    "        \"\"\"åŸºäºåˆ†æç»“æœæ¨æµ‹ç‹—çš„æƒ³æ³•å’Œå¿ƒæƒ…\"\"\"\n",
    "        \n",
    "        emotion = analysis['dominant_emotion']\n",
    "        audio_behavior = analysis['audio_behavior']\n",
    "        scene_keywords = analysis['scene_keywords']\n",
    "        \n",
    "        # è§„åˆ™åŸºç¡€çš„æ¨æµ‹\n",
    "        thoughts = {\n",
    "            'mood': 'neutral',\n",
    "            'likely_thoughts': [],\n",
    "            'behavioral_interpretation': '',\n",
    "            'needs_attention': False\n",
    "        }\n",
    "        \n",
    "        # åŸºäºæƒ…ç»ªå’ŒéŸ³é¢‘è¡Œä¸ºçš„æ¨æµ‹\n",
    "        if emotion == 'happy' and audio_behavior in ['Excited', 'Demand']:\n",
    "            thoughts['mood'] = 'playful'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I want to play!',\n",
    "                'This is fun!',\n",
    "                'Pay attention to me!'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog appears to be in a playful mood and seeking interaction.'\n",
    "            \n",
    "        elif emotion == 'sad' or audio_behavior in ['Lonely', 'Pain']:\n",
    "            thoughts['mood'] = 'distressed'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I feel lonely',\n",
    "                'I need comfort',\n",
    "                'Something is bothering me'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog may be experiencing distress and needs comfort or attention.'\n",
    "            thoughts['needs_attention'] = True\n",
    "            \n",
    "        elif audio_behavior in ['Alert', 'Territorial']:\n",
    "            thoughts['mood'] = 'vigilant'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'Something is happening',\n",
    "                'I need to protect my territory',\n",
    "                'Alert! Someone is coming'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog is in alert mode, possibly responding to external stimuli.'\n",
    "            \n",
    "        elif emotion == 'angry' or audio_behavior == 'Aggressive':\n",
    "            thoughts['mood'] = 'defensive'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I feel threatened',\n",
    "                'Stay away from me',\n",
    "                'I need to defend myself'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog is displaying defensive or aggressive behavior.'\n",
    "            thoughts['needs_attention'] = True\n",
    "            \n",
    "        elif emotion == 'calm':\n",
    "            thoughts['mood'] = 'relaxed'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I feel comfortable',\n",
    "                'Everything is peaceful',\n",
    "                'I am content'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog appears to be in a relaxed and comfortable state.'\n",
    "            \n",
    "        # å¤„ç†æœªçŸ¥æƒ…ç»ªæˆ–å…¶ä»–æƒ…ç»ªç±»åˆ«\n",
    "        elif emotion and emotion not in ['unknown', 'neutral']:\n",
    "            thoughts['mood'] = emotion\n",
    "            thoughts['likely_thoughts'] = [f'I am feeling {emotion}']\n",
    "            thoughts['behavioral_interpretation'] = f'The dog is displaying {emotion} behavior.'\n",
    "            \n",
    "        else:\n",
    "            thoughts['mood'] = 'neutral'\n",
    "            thoughts['likely_thoughts'] = ['Observing the environment']\n",
    "            thoughts['behavioral_interpretation'] = 'The dog is in a neutral state, observing its surroundings.'\n",
    "        \n",
    "        # åŸºäºåœºæ™¯å…³é”®è¯çš„é¢å¤–æ¨æµ‹\n",
    "        if 'food' in scene_keywords or 'eating' in scene_keywords:\n",
    "            thoughts['likely_thoughts'].append('Food! I want some!')\n",
    "            \n",
    "        if 'person' in scene_keywords or 'human' in scene_keywords:\n",
    "            thoughts['likely_thoughts'].append('My human is here!')\n",
    "            \n",
    "        if 'outside' in scene_keywords or 'park' in scene_keywords:\n",
    "            thoughts['likely_thoughts'].append('Time for adventure!')\n",
    "        \n",
    "        return thoughts\n",
    "    \n",
    "    def analyze_video(self, video_path: str) -> Dict:\n",
    "        \"\"\"ä¸»å‡½æ•°ï¼šåˆ†æè§†é¢‘\"\"\"\n",
    "        print(f\"[INFO] Starting video analysis for: {video_path}\")\n",
    "        \n",
    "        # 1. æå–è§†é¢‘ç»„ä»¶\n",
    "        frames, audio = self.extract_video_components(video_path)\n",
    "        \n",
    "        # 2. åˆ†æåœºæ™¯\n",
    "        scene_descriptions = self.analyze_scenes(frames)\n",
    "        \n",
    "        # 3. åˆ†ææƒ…ç»ª\n",
    "        emotion_results = self.analyze_emotions(frames)\n",
    "        \n",
    "        # 4. åˆ†æéŸ³é¢‘\n",
    "        audio_result = self.analyze_audio(audio)\n",
    "        \n",
    "        # 5. ç»¼åˆåˆ†æ\n",
    "        final_analysis = self.synthesize_analysis(\n",
    "            scene_descriptions, emotion_results, audio_result\n",
    "        )\n",
    "        \n",
    "        print(\"[INFO] Video analysis completed!\")\n",
    "        return final_analysis\n",
    "    \n",
    "    def save_results(self, results: Dict, output_path: str):\n",
    "        \"\"\"ä¿å­˜åˆ†æç»“æœ\"\"\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[INFO] Results saved to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åº\"\"\"\n",
    "    # åˆå§‹åŒ–åˆ†æç³»ç»Ÿ\n",
    "    analyzer = VideoAnalysisSystem()\n",
    "    \n",
    "    # åˆ†æè§†é¢‘\n",
    "    video_path = \"dog.mp4\"  # æ›¿æ¢ä¸ºæ‚¨çš„è§†é¢‘è·¯å¾„\n",
    "    \n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"[ERROR] Video file not found: {video_path}\")\n",
    "        print(\"[INFO] Please make sure the video file exists or update the video_path variable\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # æ‰§è¡Œåˆ†æ\n",
    "        results = analyzer.analyze_video(video_path)\n",
    "        \n",
    "        # æ‰“å°ç»“æœæ‘˜è¦\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ğŸ• DOG ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Mood: {results['dog_thoughts']['mood']}\")\n",
    "        print(f\"Dominant Emotion: {results['dominant_emotion']}\")\n",
    "        print(f\"Audio Behavior: {results['audio_behavior']}\")\n",
    "        print(f\"Needs Attention: {results['dog_thoughts']['needs_attention']}\")\n",
    "        print(f\"Behavioral Interpretation: {results['dog_thoughts']['behavioral_interpretation']}\")\n",
    "        print(\"\\nLikely Thoughts:\")\n",
    "        for thought in results['dog_thoughts']['likely_thoughts']:\n",
    "            print(f\"  - {thought}\")\n",
    "        \n",
    "        # ä¿å­˜å®Œæ•´ç»“æœ\n",
    "        analyzer.save_results(results, \"dog_analysis_results.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred during analysis: {e}\")\n",
    "        print(\"[INFO] Please check your video file and model files\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7899561-eab0-4d91-ac6e-48146ef718fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Loading BLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:17<00:00, 68.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading CLAP model...\n",
      "[INFO] Loading ResNet18 model...\n",
      "[INFO] Detected 4 emotion classes from checkpoint\n",
      "[INFO] Loaded pre-trained emotion model\n",
      "[INFO] Loading LLM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting video analysis for: dog.mp4\n",
      "[INFO] Extracting components from dog.mp4\n",
      "[INFO] Extracted 3 frames\n",
      "[WARNING] Could not extract audio: \n",
      "[INFO] Analyzing scenes with BLIP...\n",
      "[INFO] Frame 1:  and\n",
      "[INFO] Frame 2:  and\n",
      "[INFO] Frame 3:  and\n",
      "[INFO] Analyzing emotions with ResNet18...\n",
      "[INFO] Frame 1: happy (0.855)\n",
      "[INFO] Frame 2: happy (0.816)\n",
      "[INFO] Frame 3: happy (0.976)\n",
      "[INFO] Analyzing audio with CLAP...\n",
      "[INFO] Synthesizing analysis...\n",
      "[INFO] Video analysis completed!\n",
      "\n",
      "==================================================\n",
      "ğŸ• DOG ANALYSIS SUMMARY\n",
      "==================================================\n",
      "Mood: happy\n",
      "Dominant Emotion: happy\n",
      "Audio Behavior: Silent\n",
      "Needs Attention: False\n",
      "Behavioral Interpretation: The dog is displaying happy behavior.\n",
      "\n",
      "Likely Thoughts:\n",
      "  - I am feeling happy\n",
      "[INFO] Results saved to dog_analysis_results.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig, \n",
    "    Blip2Processor, \n",
    "    Blip2ForConditionalGeneration,\n",
    "    ClapModel, \n",
    "    ClapProcessor,\n",
    "    pipeline\n",
    ")\n",
    "from torchvision import transforms, models\n",
    "from torch import nn\n",
    "import tempfile\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import subprocess\n",
    "\n",
    "class VideoAnalysisSystem:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"[INFO] Using device: {self.device}\")\n",
    "        \n",
    "        # åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹\n",
    "        self._init_blip_model()\n",
    "        self._init_clap_model()\n",
    "        self._init_resnet_model()\n",
    "        self._init_llm_model()\n",
    "        \n",
    "    def _init_blip_model(self):\n",
    "        print(\"[INFO] Loading BLIP model...\")\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True,\n",
    "        )\n",
    "        \n",
    "        self.blip_processor = Blip2Processor.from_pretrained(\n",
    "            \"Salesforce/blip2-opt-2.7b\", use_fast=True\n",
    "        )\n",
    "        self.blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/blip2-opt-2.7b\",\n",
    "            quantization_config=quant_config,\n",
    "            device_map=\"auto\",\n",
    "            offload_folder=\"offload\",\n",
    "            offload_state_dict=True\n",
    "        )\n",
    "        self.blip_model.eval()\n",
    "\n",
    "        \n",
    "    def _init_clap_model(self):\n",
    "        print(\"[INFO] Loading CLAP model...\")\n",
    "        self.clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-fused\").to(self.device)\n",
    "        self.clap_processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n",
    "        \n",
    "        self.audio_prompts = {\n",
    "            \"Alert\": [\n",
    "                \"sharp mid-pitch alert bark\",\n",
    "                \"brief clear alarm bark\",\n",
    "                \"short crisp warning bark\",\n",
    "                \"fast staccato alert bark\"\n",
    "            ],\n",
    "            \"Territorial\": [\n",
    "                \"deep sustained territorial bark\",\n",
    "                \"low throaty guard bark\",\n",
    "                \"prolonged defensive bark\",\n",
    "                \"slow booming territorial bark\"\n",
    "            ],\n",
    "            \"Excited\": [\n",
    "                \"high-pitched rapid excited yips\",\n",
    "                \"quick playful bark\",\n",
    "                \"series of lively yips\",\n",
    "                \"fast bright excited bark\"\n",
    "            ],\n",
    "            \"Demand\": [\n",
    "                \"steady attention-seeking bark\",\n",
    "                \"regular rhythmic demand bark\",\n",
    "                \"persistent repetitive request bark\",\n",
    "                \"moderate-pitch insistence bark\"\n",
    "            ],\n",
    "            \"Fear\": [\n",
    "                \"high-pitched trembling fearful bark\",\n",
    "                \"quivering anxious bark\",\n",
    "                \"shaky high anxious bark\",\n",
    "                \"piercing nervous bark\"\n",
    "            ],\n",
    "            \"Aggressive\": [\n",
    "                \"low guttural aggressive bark\",\n",
    "                \"deep harsh threat bark\",\n",
    "                \"rough menacing bark\",\n",
    "                \"raspy growling attack bark\"\n",
    "            ],\n",
    "            \"Pain\": [\n",
    "                \"single sharp pain yelp\",\n",
    "                \"shrill acute pain bark\",\n",
    "                \"sudden high pain yelp\",\n",
    "                \"short piercing pain bark\"\n",
    "            ],\n",
    "            \"Lonely\": [\n",
    "                \"slow spaced lonely bark\",\n",
    "                \"mournful drawn-out bark\",\n",
    "                \"distant monotone lonely bark\",\n",
    "                \"long-interval melancholy bark\"\n",
    "            ],\n",
    "            \"Howl\": [\n",
    "                \"long plaintive canine howl\",\n",
    "                \"careless, spontaneous howl\",\n",
    "                \"sustained mournful howl\",\n",
    "                \"extended melodic dog howl\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        all_prompts, prompt_cls = [], []\n",
    "        for cls, plist in self.audio_prompts.items():\n",
    "            all_prompts.extend(plist)\n",
    "            prompt_cls.extend([cls] * len(plist))\n",
    "        \n",
    "        self.prompt_cls = np.array(prompt_cls)\n",
    "        txt_inputs = self.clap_processor(text=all_prompts, return_tensors=\"pt\", padding=True)\n",
    "        txt_inputs = {k: v.to(self.device) for k, v in txt_inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.text_emb = torch.nn.functional.normalize(\n",
    "                self.clap_model.get_text_features(**txt_inputs), dim=-1\n",
    "            )\n",
    "        \n",
    "    def _init_resnet_model(self):\n",
    "        print(\"[INFO] Loading ResNet18 model...\")\n",
    "        self.resnet_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        if os.path.exists('dog_emotion.pth'):\n",
    "            try:\n",
    "                checkpoint = torch.load('dog_emotion.pth', map_location=self.device)\n",
    "                \n",
    "                if 'fc.weight' in checkpoint:\n",
    "                    num_classes = checkpoint['fc.weight'].shape[0]\n",
    "                    print(f\"[INFO] Detected {num_classes} emotion classes from checkpoint\")\n",
    "                else:\n",
    "                    num_classes = 4\n",
    "                    print(f\"[INFO] Using default {num_classes} emotion classes\")\n",
    "                \n",
    "                if num_classes == 4:\n",
    "                    emotion_classes = ['angry', 'happy', 'relaxed', 'sad']\n",
    "                elif num_classes == 5:\n",
    "                    emotion_classes = ['angry', 'happy', 'relaxed', 'sad', 'calm']\n",
    "                else:\n",
    "                    emotion_classes = [f'emotion_{i}' for i in range(num_classes)]\n",
    "                \n",
    "                self.emotion_classes = emotion_classes\n",
    "                self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, num_classes)\n",
    "                \n",
    "                self.resnet_model.load_state_dict(checkpoint)\n",
    "                print(\"[INFO] Loaded pre-trained emotion model\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Error loading emotion model: {e}\")\n",
    "                print(\"[INFO] Using default emotion classes\")\n",
    "                emotion_classes = ['angry', 'happy', 'relaxed', 'sad']\n",
    "                self.emotion_classes = emotion_classes\n",
    "                self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, len(emotion_classes))\n",
    "        else:\n",
    "            print(\"[INFO] No pre-trained emotion model found, using default classes\")\n",
    "            emotion_classes = ['angry', 'happy', 'relaxed', 'sad']\n",
    "            self.emotion_classes = emotion_classes\n",
    "            self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, len(emotion_classes))\n",
    "        \n",
    "        self.resnet_model = self.resnet_model.to(self.device)\n",
    "        self.resnet_model.eval()\n",
    "        \n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def _init_llm_model(self):\n",
    "        print(\"[INFO] Loading LLM model...\")\n",
    "        try:\n",
    "            self.llm = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=\"microsoft/DialoGPT-medium\",\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "        except:\n",
    "            print(\"[WARNING] Could not load LLM model, will use rule-based analysis\")\n",
    "            self.llm = None\n",
    "    \n",
    "    def extract_video_components(self, video_path: str, sample_rate: int = 48000) -> Tuple[List[np.ndarray], np.ndarray]:\n",
    "        print(f\"[INFO] Extracting components from {video_path}\")\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames <= 0:\n",
    "            print(\"[WARNING] Could not get total frame count, reading sequentially.\")\n",
    "            for _ in range(3):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame_rgb)\n",
    "        else:\n",
    "            indices = [0, total_frames // 2, total_frames - 1]\n",
    "            unique_indices = sorted(set(indices))\n",
    "            for idx in unique_indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"[WARNING] Could not read frame at index {idx}\")\n",
    "                    continue\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame_rgb)\n",
    "        \n",
    "        cap.release()\n",
    "        print(f\"[INFO] Extracted {len(frames)} frames\")\n",
    "        \n",
    "        try:\n",
    "            audio, sr = librosa.load(video_path, sr=sample_rate, mono=True)\n",
    "            print(f\"[INFO] Extracted audio: {len(audio)} samples at {sr} Hz\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not extract audio: {e}\")\n",
    "            audio = np.array([])\n",
    "        \n",
    "        return frames, audio\n",
    "\n",
    "    \n",
    "    def analyze_scenes(self, frames: List[np.ndarray]) -> List[str]:\n",
    "        print(\"[INFO] Analyzing scenes with BLIP...\")\n",
    "        scene_descriptions = []\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            try:\n",
    "                img = Image.fromarray(frame).convert(\"RGB\")\n",
    "                \n",
    "                inputs = self.blip_processor(images=img, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.blip_model.device) for k, v in inputs.items()}\n",
    "                \n",
    "                generated_ids = self.blip_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=80,\n",
    "                    num_beams=5,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                )\n",
    "                \n",
    "                description = self.blip_processor.batch_decode(\n",
    "                    generated_ids, skip_special_tokens=True\n",
    "                )[0]\n",
    "                \n",
    "                scene_descriptions.append(description)\n",
    "                print(f\"[INFO] Frame {i+1}: {description}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Error analyzing frame {i+1}: {e}\")\n",
    "                scene_descriptions.append(\"Unable to analyze scene\")\n",
    "                \n",
    "        return scene_descriptions\n",
    "    \n",
    "    def analyze_emotions(self, frames: List[np.ndarray]) -> List[Dict]:\n",
    "        print(\"[INFO] Analyzing emotions with ResNet18...\")\n",
    "        emotion_results = []\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            try:\n",
    "                # è½¬æ¢ä¸ºPIL Imageå¹¶é¢„å¤„ç†\n",
    "                img = Image.fromarray(frame).convert(\"RGB\")\n",
    "                img_tensor = self.image_transform(img).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.resnet_model(img_tensor)\n",
    "                    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "                    \n",
    "                    # è·å–é¢„æµ‹ç»“æœ\n",
    "                    pred_idx = torch.argmax(probabilities).item()\n",
    "                    confidence = probabilities[pred_idx].item()\n",
    "                    \n",
    "                    emotion_result = {\n",
    "                        'emotion': self.emotion_classes[pred_idx],\n",
    "                        'confidence': confidence,\n",
    "                        'all_probabilities': {\n",
    "                            emotion: prob.item() \n",
    "                            for emotion, prob in zip(self.emotion_classes, probabilities)\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    emotion_results.append(emotion_result)\n",
    "                    print(f\"[INFO] Frame {i+1}: {emotion_result['emotion']} ({confidence:.3f})\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Error analyzing emotion for frame {i+1}: {e}\")\n",
    "                emotion_results.append({\n",
    "                    'emotion': 'unknown',\n",
    "                    'confidence': 0.0,\n",
    "                    'all_probabilities': {}\n",
    "                })\n",
    "                \n",
    "        return emotion_results\n",
    "    \n",
    "    def analyze_audio(self, audio: np.ndarray, sample_rate: int = 48000) -> Dict:\n",
    "        print(\"[INFO] Analyzing audio with CLAP...\")\n",
    "        \n",
    "        if len(audio) == 0:\n",
    "            return {\n",
    "                'predicted_class': 'Silent',\n",
    "                'confidence': 0.0,\n",
    "                'all_probabilities': {}\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # åˆ†æ®µå¤„ç†éŸ³é¢‘\n",
    "            seg_seconds = 10\n",
    "            seg_len = seg_seconds * sample_rate\n",
    "            segments = [audio[i:i+seg_len] for i in range(0, len(audio), seg_len)]\n",
    "            seg_sims = []\n",
    "            \n",
    "            for seg in segments:\n",
    "                if len(seg) < 1000:\n",
    "                    continue\n",
    "                    \n",
    "                a_in = self.clap_processor(\n",
    "                    audios=[seg], \n",
    "                    sampling_rate=sample_rate,\n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True\n",
    "                )\n",
    "                a_in = {k: v.to(self.device) for k, v in a_in.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    a_emb = torch.nn.functional.normalize(\n",
    "                        self.clap_model.get_audio_features(**a_in), dim=-1\n",
    "                    )\n",
    "                    \n",
    "                sim = (a_emb @ self.text_emb.T).softmax(dim=-1)[0].cpu().numpy()\n",
    "                seg_sims.append(sim)\n",
    "            \n",
    "            if not seg_sims:\n",
    "                seg_sims = [np.zeros(len(self.text_emb))]\n",
    "            \n",
    "            sim_avg = np.mean(np.vstack(seg_sims), axis=0)\n",
    "            \n",
    "            cls_prob = {}\n",
    "            for cls in self.audio_prompts:\n",
    "                cls_prob[cls] = float(np.mean(sim_avg[self.prompt_cls == cls]))\n",
    "            \n",
    "            pred_cls = max(cls_prob, key=cls_prob.get)\n",
    "            confidence = cls_prob[pred_cls]\n",
    "            \n",
    "            print(f\"[INFO] Audio analysis: {pred_cls} ({confidence:.3f})\")\n",
    "            \n",
    "            return {\n",
    "                'predicted_class': pred_cls,\n",
    "                'confidence': confidence,\n",
    "                'all_probabilities': cls_prob\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Error analyzing audio: {e}\")\n",
    "            return {\n",
    "                'predicted_class': 'Unknown',\n",
    "                'confidence': 0.0,\n",
    "                'all_probabilities': {}\n",
    "            }\n",
    "    \n",
    "    def synthesize_analysis(self, scene_descriptions: List[str], \n",
    "                          emotion_results: List[Dict], \n",
    "                          audio_result: Dict) -> Dict:\n",
    "        print(\"[INFO] Synthesizing analysis...\")\n",
    "        \n",
    "        emotions = [result['emotion'] for result in emotion_results if result['emotion'] != 'unknown']\n",
    "        most_common_emotion = max(set(emotions), key=emotions.count) if emotions else 'unknown'\n",
    "        \n",
    "        avg_emotion_confidence = np.mean([\n",
    "            result['confidence'] for result in emotion_results \n",
    "            if result['confidence'] > 0\n",
    "        ]) if emotion_results else 0.0\n",
    "        \n",
    "        scene_keywords = []\n",
    "        for desc in scene_descriptions:\n",
    "            keywords = [word.lower() for word in desc.split() \n",
    "                       if len(word) > 3 and word.lower() not in ['the', 'and', 'with', 'that', 'this']]\n",
    "            scene_keywords.extend(keywords)\n",
    "        \n",
    "        analysis = {\n",
    "            'dominant_emotion': most_common_emotion,\n",
    "            'emotion_confidence': avg_emotion_confidence,\n",
    "            'audio_behavior': audio_result['predicted_class'],\n",
    "            'audio_confidence': audio_result['confidence'],\n",
    "            'scene_context': scene_descriptions,\n",
    "            'scene_keywords': list(set(scene_keywords)),\n",
    "            'detailed_emotions': emotion_results,\n",
    "            'audio_details': audio_result\n",
    "        }\n",
    "        \n",
    "        dog_analysis = self._generate_dog_thoughts(analysis)\n",
    "        analysis['dog_thoughts'] = dog_analysis\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _generate_dog_thoughts(self, analysis: Dict) -> Dict:\n",
    "        emotion = analysis['dominant_emotion']\n",
    "        audio_behavior = analysis['audio_behavior']\n",
    "        scene_keywords = analysis['scene_keywords']\n",
    "        \n",
    "        thoughts = {\n",
    "            'mood': 'neutral',\n",
    "            'likely_thoughts': [],\n",
    "            'behavioral_interpretation': '',\n",
    "            'needs_attention': False\n",
    "        }\n",
    "        \n",
    "        if emotion == 'happy' and audio_behavior in ['Excited', 'Demand']:\n",
    "            thoughts['mood'] = 'playful'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I want to play!',\n",
    "                'This is fun!',\n",
    "                'Pay attention to me!'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog appears to be in a playful mood and seeking interaction.'\n",
    "            \n",
    "        elif emotion == 'sad' or audio_behavior in ['Lonely', 'Pain']:\n",
    "            thoughts['mood'] = 'distressed'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I feel lonely',\n",
    "                'I need comfort',\n",
    "                'Something is bothering me'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog may be experiencing distress and needs comfort or attention.'\n",
    "            thoughts['needs_attention'] = True\n",
    "            \n",
    "        elif audio_behavior in ['Alert', 'Territorial']:\n",
    "            thoughts['mood'] = 'vigilant'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'Something is happening',\n",
    "                'I need to protect my territory',\n",
    "                'Alert! Someone is coming'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog is in alert mode, possibly responding to external stimuli.'\n",
    "            \n",
    "        elif emotion == 'angry' or audio_behavior == 'Aggressive':\n",
    "            thoughts['mood'] = 'defensive'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I feel threatened',\n",
    "                'Stay away from me',\n",
    "                'I need to defend myself'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog is displaying defensive or aggressive behavior.'\n",
    "            thoughts['needs_attention'] = True\n",
    "            \n",
    "        elif emotion == 'calm':\n",
    "            thoughts['mood'] = 'relaxed'\n",
    "            thoughts['likely_thoughts'] = [\n",
    "                'I feel comfortable',\n",
    "                'Everything is peaceful',\n",
    "                'I am content'\n",
    "            ]\n",
    "            thoughts['behavioral_interpretation'] = 'The dog appears to be in a relaxed and comfortable state.'\n",
    "            \n",
    "        elif emotion and emotion not in ['unknown', 'neutral']:\n",
    "            thoughts['mood'] = emotion\n",
    "            thoughts['likely_thoughts'] = [f'I am feeling {emotion}']\n",
    "            thoughts['behavioral_interpretation'] = f'The dog is displaying {emotion} behavior.'\n",
    "            \n",
    "        else:\n",
    "            thoughts['mood'] = 'neutral'\n",
    "            thoughts['likely_thoughts'] = ['Observing the environment']\n",
    "            thoughts['behavioral_interpretation'] = 'The dog is in a neutral state, observing its surroundings.'\n",
    "        \n",
    "        if 'food' in scene_keywords or 'eating' in scene_keywords:\n",
    "            thoughts['likely_thoughts'].append('Food! I want some!')\n",
    "            \n",
    "        if 'person' in scene_keywords or 'human' in scene_keywords:\n",
    "            thoughts['likely_thoughts'].append('My human is here!')\n",
    "            \n",
    "        if 'outside' in scene_keywords or 'park' in scene_keywords:\n",
    "            thoughts['likely_thoughts'].append('Time for adventure!')\n",
    "        \n",
    "        return thoughts\n",
    "    \n",
    "    def analyze_video(self, video_path: str) -> Dict:\n",
    "        print(f\"[INFO] Starting video analysis for: {video_path}\")\n",
    "        \n",
    "        frames, audio = self.extract_video_components(video_path)\n",
    "        \n",
    "        scene_descriptions = self.analyze_scenes(frames)\n",
    "        \n",
    "        emotion_results = self.analyze_emotions(frames)\n",
    "        \n",
    "        audio_result = self.analyze_audio(audio)\n",
    "        \n",
    "        final_analysis = self.synthesize_analysis(\n",
    "            scene_descriptions, emotion_results, audio_result\n",
    "        )\n",
    "        \n",
    "        print(\"[INFO] Video analysis completed!\")\n",
    "        return final_analysis\n",
    "    \n",
    "    def save_results(self, results: Dict, output_path: str):\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[INFO] Results saved to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    analyzer = VideoAnalysisSystem()\n",
    "\n",
    "    video_path = \"dog.mp4\"\n",
    "    \n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"[ERROR] Video file not found: {video_path}\")\n",
    "        print(\"[INFO] Please make sure the video file exists or update the video_path variable\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        results = analyzer.analyze_video(video_path)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ğŸ• DOG ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Mood: {results['dog_thoughts']['mood']}\")\n",
    "        print(f\"Dominant Emotion: {results['dominant_emotion']}\")\n",
    "        print(f\"Audio Behavior: {results['audio_behavior']}\")\n",
    "        print(f\"Needs Attention: {results['dog_thoughts']['needs_attention']}\")\n",
    "        print(f\"Behavioral Interpretation: {results['dog_thoughts']['behavioral_interpretation']}\")\n",
    "        print(\"\\nLikely Thoughts:\")\n",
    "        for thought in results['dog_thoughts']['likely_thoughts']:\n",
    "            print(f\"  - {thought}\")\n",
    "\n",
    "        analyzer.save_results(results, \"dog_analysis_results.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred during analysis: {e}\")\n",
    "        print(\"[INFO] Please check your video file and model files\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c88613-7716-41b0-ad85-b2ccf6df6e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
